{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole: A Comparison\n",
    "This Mini-project's goal is to apply Deep Reinforcement Learning techniques to the Cart-Pole environment. We will implement DQN (REF) as well as improvements Double DQN, and Dueling DQN. The performance of all three will be compared on cartpole. This will hopefully act as a test bed for understanding various improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open AI Gym\n",
    "First I want to import Open AI gym and test that the cartpole environment will work. I will play a game with the policy that the left action will be chosen whenever the velocity of the pole is to the right and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    # This policy looks at the angular velocity (state[3]) and applies force in the opposite direction.\n",
    "    # It is by no means a perfect policy but it is a decent test to ensure its working as expected\n",
    "    action = 0 if (state[3] < 0) else 1\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q Learning (DQN)\n",
    "The agent below implements DQN as presented in [REF]. Since CartPole is a simple game, The network will be implemented with only 2 hidden layers. The orignal DQN paper introduced two key ideas. 1) The replay-buffer which records (S,A,R,S') experiences and 2) The target of the Q learning (approximation of true Q value) is held fixed and periodically updated.\n",
    "\n",
    "In order to implement 2) we need to create two identical networks and then copy operations for moving the parameters from one to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replay Buffer Class by David Kroezen\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "\n",
    "    num_state = 3\n",
    "    \n",
    "    def __init__(self, buffer_size):\n",
    "        \" Initializes the replay buffer by creating a deque() and setting the size and buffer count. \"\n",
    "        self.buffer = deque()\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "         \n",
    "    def add(self, s, a, r, d, s2):\n",
    "         \n",
    "        \"\"\" Adds new experience to the ReplayBuffer(). If the buffer size is\n",
    "        reached, the oldest item is removed.\n",
    "         \n",
    "        Inputs needed to create new experience:\n",
    "            s      - State\n",
    "            a      - Action\n",
    "            r      - Reward\n",
    "            d      - Done\n",
    "            s2     - Resulting State     \n",
    "        \"\"\"\n",
    "        d = 1 if d else 0\n",
    "        # Create experience list\n",
    "        experience = (s, a, r, d, s2)\n",
    "        \n",
    "        # Check the size of the buffer\n",
    "        if self.count < self.buffer_size:\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            \n",
    "        # Add experience to buffer\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def size(self):\n",
    "        \" Return the amount of stored experiences. \" \n",
    "        return self.count\n",
    "    \n",
    "    def batch(self, batch_size):\n",
    "        \"Return a \\\"batch_size\\\" number of random samples from the buffer.\"\n",
    "        \n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "            batch_size = self.count\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "            \n",
    "        batch_state = np.array([item[0] for item in batch])#.reshape([batch_size,self.num_state])\n",
    "        batch_action = np.array([item[1] for item in batch])#.reshape([batch_size, 1])\n",
    "        batch_reward = np.array([item[2] for item in batch])#.reshape([batch_size, 1])\n",
    "        batch_done = np.array([item[3] for item in batch])#.reshape([batch_size, 1])\n",
    "        batch_next_state = np.array([item[4] for item in batch])#.reshape([batch_size,self.num_state])\n",
    "        \n",
    "        return batch_state, batch_action, batch_reward, batch_done, batch_next_state \n",
    "            \n",
    "    def clear(self):\n",
    "        \" Remove all entries from the ReplayBuffer. \"\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeDQN(state, h_size, a_size, name):\n",
    "    # Make a network\n",
    "    with tf.variable_scope(name):\n",
    "        h1 = tf.nn.relu(tf.layers.dense(state, h_size))\n",
    "        h2 = tf.nn.relu(tf.layers.dense(h1, h_size))\n",
    "        out = tf.layers.dense(h2, a_size)\n",
    "    return out\n",
    "\n",
    "def copyVars(fromName, toName):\n",
    "    # Constructs the copy operations\n",
    "    fvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=fromName)\n",
    "    tvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=toName)\n",
    "    \n",
    "    copy = [tf.assign(t, f) for f, t in zip(fvars, tvars)]\n",
    "    return copy\n",
    "    \n",
    "\n",
    "class DQN():\n",
    "    def __init__(self, sess, e_size, h_size, a_size, lr=0.01, gamma=0.99, replay_length=10000, batch_size=64):\n",
    "        \n",
    "        # Store params\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_length = replay_length\n",
    "        self.sess = sess\n",
    "        \n",
    "        # Define Inputs\n",
    "        self.state = tf.placeholder(tf.float32, shape=[None, e_size], name='State_input')\n",
    "        self.actions = tf.placeholder(tf.int32, shape=[None], name='actions')\n",
    "        self.rewards = tf.placeholder(tf.float32, shape=[None], name='rewards')\n",
    "#         self.dones = tf.placeholder(tf.float32, shape=[None], name='dones')\n",
    "        self.next_state = tf.placeholder(tf.float32, shape=[None, e_size], name='Next_State_input')\n",
    "#         self.learning_rate = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # Define Network\n",
    "        self.main = makeDQN(self.state, h_size, a_size, 'main')\n",
    "        self.target = makeDQN(self.next_state, h_size, a_size, 'target')\n",
    "        \n",
    "        # Create the update operations for updating the target network\n",
    "        self.copy = copyVars('main', 'target')\n",
    "        \n",
    "        # Define the Loss operation \n",
    "        target = self.rewards + gamma*tf.reduce_max(self.target, axis=1)\n",
    "        choice = tf.one_hot(self.actions, a_size)\n",
    "        predicted = tf.reduce_max(self.main*choice)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(target-predicted))\n",
    "        \n",
    "        # Optimizer\n",
    "        # Restrict training to the main network\n",
    "        tvars = tf.trainable_variables(scope='main')\n",
    "        self.optimize = tf.train.AdamOptimizer(lr).minimize(self.loss, var_list=tvars)\n",
    "        \n",
    "        # Helper ops\n",
    "        self.choice = tf.argmax(self.main, 1)\n",
    "        self.choice_prob = tf.nn.softmax(self.main)\n",
    "        \n",
    "        \n",
    "        # Set up experience replay buffer\n",
    "        self.replay = ReplayBuffer(replay_length)\n",
    "        \n",
    "    def train(self):\n",
    "        # Get a batch of experiences\n",
    "        states, actions, rewards, dones, next_states = self.replay.batch(self.batch_size)\n",
    "        self.sess.run(self.optimize, feed_dict={self.state: states,\n",
    "                                            self.actions: actions,\n",
    "                                            self.rewards: rewards,\n",
    "                                            self.next_state: next_states})\n",
    "    def clean_state(self, s):\n",
    "        # This really SHOULD convert to a numpy array also\n",
    "        if (len(s.shape)==1):\n",
    "            s = s[None, :]\n",
    "        return s\n",
    "    \n",
    "    def remember(self, state, action, reward, done, next_state):\n",
    "        self.replay.add(state, action, reward, done, next_state)\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        return self.sess.run(self.choice, feed_dict={self.state: self.clean_state(state)})\n",
    "    \n",
    "    def choose_probs(self, state):\n",
    "        return self.sess.run(self.choice_prob, feed_dict={self.state: self.clean_state(state)})\n",
    "    \n",
    "    def update_target(self):\n",
    "#         print (self.sess.run(self.main, feed_dict={self.state:[[0.01,0.01,0.01,0.01]]}))\n",
    "#         print (self.sess.run(self.target, feed_dict={self.next_state:[[0.01,0.01,0.01,0.01]]}))\n",
    "        self.sess.run(self.copy)\n",
    "#         print (self.sess.run(self.target, feed_dict={self.next_state:[[0.01,0.01,0.01,0.01]]}))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running DQN\n",
    "With the agent defined its time to test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:       0 \tEpsilon:  1.00 \tTest Score:   22.25\n",
      "Round:     100 \tEpsilon:  0.80 \tTest Score:    9.47\n",
      "Round:     200 \tEpsilon:  0.68 \tTest Score:    9.44\n",
      "Round:     300 \tEpsilon:  0.58 \tTest Score:    9.56\n",
      "Round:     400 \tEpsilon:  0.50 \tTest Score:    9.38\n",
      "Round:     500 \tEpsilon:  0.44 \tTest Score:    9.38\n",
      "Round:     600 \tEpsilon:  0.39 \tTest Score:    9.47\n",
      "Round:     700 \tEpsilon:  0.35 \tTest Score:    9.44\n",
      "Round:     800 \tEpsilon:  0.31 \tTest Score:    9.44\n",
      "Round:     900 \tEpsilon:  0.28 \tTest Score:    9.69\n",
      "Round:    1000 \tEpsilon:  0.25 \tTest Score:    9.22\n",
      "Round:    1100 \tEpsilon:  0.23 \tTest Score:    9.34\n",
      "Round:    1200 \tEpsilon:  0.20 \tTest Score:    9.41\n",
      "Round:    1300 \tEpsilon:  0.18 \tTest Score:    9.28\n",
      "Round:    1400 \tEpsilon:  0.17 \tTest Score:    9.41\n",
      "Round:    1500 \tEpsilon:  0.15 \tTest Score:    9.31\n",
      "Round:    1600 \tEpsilon:  0.14 \tTest Score:    9.41\n",
      "Round:    1700 \tEpsilon:  0.13 \tTest Score:    9.12\n",
      "Round:    1800 \tEpsilon:  0.11 \tTest Score:    9.62\n",
      "Round:    1900 \tEpsilon:  0.10 \tTest Score:    9.31\n",
      "Round:    2000 \tEpsilon:  0.10 \tTest Score:    9.34\n",
      "Round:    2100 \tEpsilon:  0.09 \tTest Score:    9.69\n",
      "Round:    2200 \tEpsilon:  0.08 \tTest Score:    9.31\n",
      "Round:    2300 \tEpsilon:  0.07 \tTest Score:    9.25\n",
      "Round:    2400 \tEpsilon:  0.07 \tTest Score:    9.56\n",
      "Round:    2500 \tEpsilon:  0.06 \tTest Score:    9.22\n",
      "Round:    2600 \tEpsilon:  0.06 \tTest Score:    9.06\n",
      "Round:    2700 \tEpsilon:  0.05 \tTest Score:    9.38\n",
      "Round:    2800 \tEpsilon:  0.05 \tTest Score:    9.44\n",
      "Round:    2900 \tEpsilon:  0.05 \tTest Score:    9.31\n",
      "Round:    3000 \tEpsilon:  0.04 \tTest Score:    9.31\n",
      "Round:    3100 \tEpsilon:  0.04 \tTest Score:    9.41\n",
      "Round:    3200 \tEpsilon:  0.04 \tTest Score:    9.09\n",
      "Round:    3300 \tEpsilon:  0.03 \tTest Score:    9.34\n",
      "Round:    3400 \tEpsilon:  0.03 \tTest Score:    9.44\n",
      "Round:    3500 \tEpsilon:  0.03 \tTest Score:    9.28\n",
      "Round:    3600 \tEpsilon:  0.03 \tTest Score:    9.25\n",
      "Round:    3700 \tEpsilon:  0.03 \tTest Score:    9.53\n",
      "Round:    3800 \tEpsilon:  0.03 \tTest Score:    9.28\n",
      "Round:    3900 \tEpsilon:  0.02 \tTest Score:    9.34\n",
      "Round:    4000 \tEpsilon:  0.02 \tTest Score:    9.25\n",
      "Round:    4100 \tEpsilon:  0.02 \tTest Score:    9.34\n",
      "Round:    4200 \tEpsilon:  0.02 \tTest Score:    9.22\n",
      "Round:    4300 \tEpsilon:  0.02 \tTest Score:    9.38\n",
      "Round:    4400 \tEpsilon:  0.02 \tTest Score:    9.44\n",
      "Round:    4500 \tEpsilon:  0.02 \tTest Score:    9.41\n",
      "Round:    4600 \tEpsilon:  0.02 \tTest Score:    9.31\n",
      "Round:    4700 \tEpsilon:  0.02 \tTest Score:    9.12\n",
      "Round:    4800 \tEpsilon:  0.02 \tTest Score:    9.28\n",
      "Round:    4900 \tEpsilon:  0.02 \tTest Score:    9.31\n",
      "Round:    5000 \tEpsilon:  0.01 \tTest Score:    9.09\n",
      "Round:    5100 \tEpsilon:  0.01 \tTest Score:    9.53\n",
      "Round:    5200 \tEpsilon:  0.01 \tTest Score:    9.34\n",
      "Round:    5300 \tEpsilon:  0.01 \tTest Score:    9.47\n",
      "Round:    5400 \tEpsilon:  0.01 \tTest Score:    9.59\n",
      "Round:    5500 \tEpsilon:  0.01 \tTest Score:    9.53\n",
      "Round:    5600 \tEpsilon:  0.01 \tTest Score:    9.41\n",
      "Round:    5700 \tEpsilon:  0.01 \tTest Score:    9.28\n",
      "Round:    5800 \tEpsilon:  0.01 \tTest Score:    9.56\n",
      "Round:    5900 \tEpsilon:  0.01 \tTest Score:    9.25\n",
      "Round:    6000 \tEpsilon:  0.01 \tTest Score:    9.25\n",
      "Round:    6100 \tEpsilon:  0.01 \tTest Score:    9.16\n",
      "Round:    6200 \tEpsilon:  0.01 \tTest Score:    9.28\n",
      "Round:    6300 \tEpsilon:  0.01 \tTest Score:    9.28\n",
      "Round:    6400 \tEpsilon:  0.01 \tTest Score:    9.16\n",
      "Round:    6500 \tEpsilon:  0.01 \tTest Score:    9.34\n",
      "Round:    6600 \tEpsilon:  0.01 \tTest Score:    9.16\n",
      "Round:    6700 \tEpsilon:  0.01 \tTest Score:    9.44\n",
      "Round:    6800 \tEpsilon:  0.01 \tTest Score:    9.22\n",
      "Round:    6900 \tEpsilon:  0.01 \tTest Score:    9.28\n",
      "Round:    7000 \tEpsilon:  0.01 \tTest Score:    9.31\n",
      "Round:    7100 \tEpsilon:  0.01 \tTest Score:    9.31\n",
      "Round:    7200 \tEpsilon:  0.01 \tTest Score:    9.44\n",
      "Round:    7300 \tEpsilon:  0.01 \tTest Score:    9.22\n",
      "Round:    7400 \tEpsilon:  0.01 \tTest Score:    9.09\n",
      "Round:    7500 \tEpsilon:  0.01 \tTest Score:    9.44\n",
      "Round:    7600 \tEpsilon:  0.01 \tTest Score:    9.44\n",
      "Round:    7700 \tEpsilon:  0.01 \tTest Score:    9.44\n",
      "Round:    7800 \tEpsilon:  0.01 \tTest Score:    9.41\n",
      "Round:    7900 \tEpsilon:  0.01 \tTest Score:    9.53\n",
      "Round:    8000 \tEpsilon:  0.01 \tTest Score:    9.25\n",
      "Round:    8100 \tEpsilon:  0.01 \tTest Score:    9.22\n",
      "Round:    8200 \tEpsilon:  0.01 \tTest Score:    9.34\n",
      "Round:    8300 \tEpsilon:  0.01 \tTest Score:    9.25\n",
      "Round:    8400 \tEpsilon:  0.01 \tTest Score:    9.28\n",
      "Round:    8500 \tEpsilon:  0.01 \tTest Score:    9.34\n",
      "Round:    8600 \tEpsilon:  0.01 \tTest Score:    9.59\n",
      "Round:    8700 \tEpsilon:  0.01 \tTest Score:    9.25\n",
      "Round:    8800 \tEpsilon:  0.01 \tTest Score:    9.44\n",
      "Round:    8900 \tEpsilon:  0.01 \tTest Score:    8.97\n",
      "Round:    9000 \tEpsilon:  0.01 \tTest Score:    9.34\n",
      "Round:    9100 \tEpsilon:  0.01 \tTest Score:    9.34\n",
      "Round:    9200 \tEpsilon:  0.01 \tTest Score:    9.22\n",
      "Round:    9300 \tEpsilon:  0.01 \tTest Score:    9.50\n",
      "Round:    9400 \tEpsilon:  0.01 \tTest Score:    9.19\n",
      "Round:    9500 \tEpsilon:  0.01 \tTest Score:    9.50\n",
      "Round:    9600 \tEpsilon:  0.01 \tTest Score:    9.59\n",
      "Round:    9700 \tEpsilon:  0.01 \tTest Score:    9.12\n",
      "Round:    9800 \tEpsilon:  0.01 \tTest Score:    9.25\n",
      "Round:    9900 \tEpsilon:  0.01 \tTest Score:    9.34\n",
      "Round:    9999 \tEpsilon:  0.01 \tTest Score:    9.50\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a0cc9f9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.0001\n",
    "buffer_size = 10000\n",
    "batch_size = 20\n",
    "gamma = 0.99\n",
    "hidden_layer = 64\n",
    "# epsilon = 0.5\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "num_episodes = 10000\n",
    "train_freq = 1\n",
    "target_update_freq = 10000\n",
    "\n",
    "report_freq = 100\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "# Environment Parameters\n",
    "e_size = 4\n",
    "a_size = 2\n",
    "\n",
    "\n",
    "def test_player(player, env, runs=100):\n",
    "    reward_sum = 0\n",
    "    for i in range(runs):\n",
    "        run_reward = 0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        t = 0.\n",
    "        while not done:\n",
    "            t += 1\n",
    "            action = player.choose_action(state)[0]\n",
    "            state, reward, done, info = env.step(action)\n",
    "            run_reward += reward\n",
    "            if done:\n",
    "                reward_sum += run_reward\n",
    "                break\n",
    "        env.reset()\n",
    "    return reward_sum/runs\n",
    "\n",
    "\n",
    "\n",
    "step_count = 0\n",
    "scores = []\n",
    "sess = tf.Session()\n",
    "\n",
    "# Create Agent\n",
    "player = DQN(sess, e_size, hidden_layer, a_size, learning_rate, gamma, buffer_size, batch_size)\n",
    "\n",
    "# Initialize \n",
    "sess.run(tf.global_variables_initializer())\n",
    "player.update_target()\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    t = 0\n",
    "    while not done:\n",
    "        step_count += 1\n",
    "        t += 1\n",
    "        # Epsilon greedy exploration policy\n",
    "        epsilon = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step_count) \n",
    "        if (np.random.uniform() <= epsilon):\n",
    "            action= np.random.choice(range(a_size))\n",
    "        else:\n",
    "            # Do a weighted sample to pick your action\n",
    "            action_prob = player.choose_probs(state)[0]\n",
    "#                 print (action_prob)\n",
    "            action = np.random.choice(range(a_size), p=action_prob)\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "#         reward = -1 if done else 0\n",
    "        player.remember(state, action, reward, done, new_state)\n",
    "        state = new_state\n",
    "\n",
    "        if (step_count % train_freq == 0):\n",
    "            player.train()\n",
    "\n",
    "        if (step_count % target_update_freq == 0):\n",
    "#             print (\"updating target\")\n",
    "            player.update_target()\n",
    "\n",
    "        if done:\n",
    "\n",
    "            if (i % report_freq == 0 or i == num_episodes-1):\n",
    "                score = test_player(player, env, runs=32)\n",
    "                scores.append(score)\n",
    "                print(\"Round: {0:7d} \\tEpsilon: {1:5.2f} \\tTest Score: {2:7.2f}\".format(i, epsilon, score))\n",
    "\n",
    "plt.plot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DGPU]",
   "language": "python",
   "name": "conda-env-DGPU-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
