{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI GYM Pendulum-v0 # \n",
    "---\n",
    "This notebook describes the algoritm used for the swing-up and balancing of a pendulum using an actor-critic reinforcement learning agent. The algorithm used is based on\n",
    "[DDPG](https://arxiv.org/pdf/1509.02971.pdf \"Continuous Control with Deep Reinforcement Learning (2015)\"). The algoritm is applied to the [Pendulum-v0](https://gym.openai.com/envs/Pendulum-v0/) environment of [OpenAI Gym](https://github.com/openai/gym), a simple and effective toolkit to develop and prototype Reinforcement Learning algorithms. \n",
    "\n",
    "To install and manage all requirements Anaconda or Miniconda is recommended. Use the provided yaml file to install all dependencies or install them manually in a new environment called `tf-gym` using the following commands:\n",
    "```\n",
    "conda create -n tf-gym python=3\n",
    "source activate tf-gym\n",
    "conda install numpy matplotlib jupyter notebook\n",
    "pip install --ignore-installed --upgrade tensorflow\n",
    "pip install gym \n",
    "```\n",
    "\n",
    "> **NOTE:** If you want to run tensorflow on the gpu, replace `tensorflow` with `tensorflow-gpu`. Don't forget to install `CUDA` and `cuDNN`. See the [TensorFlow](https://www.tensorflow.org/) website for more info.\n",
    "\n",
    "This DDPG approach uses batch normalization on both hidden layers. As batch normalization uses an internal moving average and variance this is not updated by the soft target network updates. Thoughts on the correctness of this is appreciated.\n",
    "\n",
    "---\n",
    "## General Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters ##\n",
    "\n",
    "The hyperparameters are a set of user set parameters which are used to setup and tune the learning process of the agent. The parameters are not able to be learned by the program itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the environment\n",
    "RENDER = False\n",
    "\n",
    "# Training epochs\n",
    "EPOCHS = 50000\n",
    "\n",
    "# Maximum number of steps per epoch\n",
    "EPOCH_LENGTH = 1000   \n",
    "\n",
    "# Number of experiences stored in the replay buffer\n",
    "BUFFER_SIZE = 1000000\n",
    "\n",
    "# Number of experiences per training batch\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Reinforcement Learning Parameters\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "LEARN_RATE_ACTOR = 0.001\n",
    "LEARN_RATE_CRITIC = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create OpenAI Gym environment ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "# Read out the expected size of the state and input\n",
    "num_states = np.prod(np.array(env.observation_space.shape)) \n",
    "num_actions = np.prod(np.array(env.action_space.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReplayBuffer Class ##\n",
    "Class to create and handel the buffer used for the replay of previous experiences. This is needed to remove temporal correlation from the state data and is shown to increase training performance of RL-Agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "\n",
    "    num_state = 3\n",
    "    \n",
    "    def __init__(self, buffer_size):\n",
    "        \" Initializes the replay buffer by creating a deque() and setting the size and buffer count. \"\n",
    "        self.buffer = deque()\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "         \n",
    "    def add(self, s, a, r, d, s2):\n",
    "         \n",
    "        \"\"\" Adds new experience to the ReplayBuffer(). If the buffer size is\n",
    "        reached, the oldest item is removed.\n",
    "         \n",
    "        Inputs needed to create new experience:\n",
    "            s      - State\n",
    "            a      - Action\n",
    "            r      - Reward\n",
    "            d      - Done\n",
    "            s2     - Resulting State     \n",
    "        \"\"\"\n",
    "        # Create experience list\n",
    "        experience = (s, a, r, d, s2)\n",
    "        \n",
    "        # Check the size of the buffer\n",
    "        if self.count < self.buffer_size:\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            \n",
    "        # Add experience to buffer\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def size(self):\n",
    "        \" Return the amount of stored experiences. \" \n",
    "        return self.count\n",
    "    \n",
    "    def batch(self, batch_size):\n",
    "        \"Return a \\\"batch_size\\\" number of random samples from the buffer.\"\n",
    "        \n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "            batch_size = self.count\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "            \n",
    "        batch_state = np.array([item[0] for item in batch]).reshape([batch_size,self.num_state])\n",
    "        batch_action = np.array([item[1] for item in batch]).reshape([batch_size, 1])\n",
    "        batch_reward = np.array([item[2] for item in batch]).reshape([batch_size, 1])\n",
    "        batch_done = np.array([item[3] for item in batch]).reshape([batch_size, 1])\n",
    "        batch_next_state = np.array([item[4] for item in batch]).reshape([batch_size,self.num_state])\n",
    "        \n",
    "        return batch_state, batch_action, batch_reward, batch_done, batch_next_state \n",
    "            \n",
    "    def clear(self):\n",
    "        \" Remove all entries from the ReplayBuffer. \"\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ActionNoise ##\n",
    "To aid the exploration of the agent, noise is added to the predicted optimal action as calculated by the Actor network. This class manages this noise input and is based on Ornstein-Uhlenbeck noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ActionNoise():\n",
    "    def __init__(self, mu = np.zeros(1), sigma= 0.3, theta = 0.15, dt = 1e-2, x0 = None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "        \n",
    "    def __call__(self):\n",
    "        x = (self.x_prev + self.theta*(self.mu - self.x_prev)*self.dt \n",
    "             + self.sigma*np.sqrt(self.dt)* np.random.normal(size=self.mu.shape))\n",
    "        self.x_prev = x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'ActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks ##\n",
    "Create the neural network classes for the actor and critic part of the reinforcement learning controller. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Neural Network ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork:\n",
    "    \n",
    "    # Actor Network Parameters\n",
    "    num_outputs = 1\n",
    "    num_hidden_1 = 400\n",
    "    num_hidden_2 = 300\n",
    "    \n",
    "    def __init__(self, session, num_states, action_range, learning_rate, tau, batch_size):\n",
    "        \" Initialize the actor and target network. \"\n",
    "        \n",
    "        # Set session\n",
    "        self.session = session\n",
    "        \n",
    "        # Set input and output parameters\n",
    "        self.num_inputs = num_states\n",
    "        self.output_min = action_range[0]\n",
    "        self.output_max = action_range[1]\n",
    "        \n",
    "        # Set learning parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Create actor and target networks\n",
    "        self.actor_input, self.actor_is_training, self.actor_network = self.make('actor')\n",
    "        self.target_input, self.target_is_training, self.target_network = self.make('target_actor', source_scope = 'actor')\n",
    "        \n",
    "        # Retrieve the collections with the variables of the actor and target networks\n",
    "        self.actor_collection = tf.get_collection('actor')\n",
    "        self.target_collection = tf.get_collection('target_actor')\n",
    "        \n",
    "        # Create update_target_network_op\n",
    "        self.update_target_network_op = [t.assign(tf.scalar_mul(self.tau, a) + tf.scalar_mul(1 - self.tau, t)) \n",
    "                                        for a, t in zip(self.actor_collection, self.target_collection)]\n",
    "\n",
    "        # Action gradient placeholder, provided by critic network\n",
    "        self.action_gradient = tf.placeholder(tf.float32, [None, 1])\n",
    "        \n",
    "        # Actor gradient with batch normalization \n",
    "        self.actor_gradients = [ (gradient/batch_size) \n",
    "                for gradient in tf.gradients(self.actor_network, self.actor_collection, -self.action_gradient) ]\n",
    "        \n",
    "        # Create optimizer\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\\\n",
    "                                    .apply_gradients(zip(self.actor_gradients, self.actor_collection)) \n",
    "\n",
    "        \n",
    "        \n",
    "    def make(self, scope, source_scope = None):\n",
    "        \" Create an actor network \"\n",
    "        \n",
    "        # Create initializer\n",
    "        if source_scope is not None: \n",
    "            init_counter = 0\n",
    "            source = tf.get_collection(source_scope)\n",
    "            \n",
    "        def initializer():\n",
    "            if source_scope = None: \n",
    "                return None\n",
    "            \n",
    "            else:\n",
    "                initializer = source[init_counter].initialized_value()\n",
    "                init_counter += 1\n",
    "                \n",
    "                return initializer \n",
    "            \n",
    "            \n",
    "        \n",
    "        # Add variable scope to easily differentiate between actor and target networks\n",
    "        with tf.variable_scope(scope): \n",
    "            \n",
    "            # Define input placeholder\n",
    "            input = tf.placeholder(tf.float32, [None,self.num_inputs])\n",
    "            \n",
    "            # Define with_default placeholder for the is_training boolean\n",
    "            is_training = tf.placeholder_with_default(False, [], name = 'is_training')\n",
    "            \n",
    "            # Create TensorFlow network\n",
    "            layer_1 = tf.layers.dense(input, self.num_hidden_1, name = 'dense_layer_1')\n",
    "            layer_1 = tf.layers.batch_normalization(layer_1, training = is_training, name = 'batch_norm_1')\n",
    "            layer_1 = tf.nn.relu(layer_1)\n",
    "            \n",
    "            layer_2 = tf.layers.dense(layer_1, self.num_hidden_2, name = 'dense_layer_2')\n",
    "            layer_2 = tf.layers.batch_normalization(layer_2, training = is_training, name = 'batch_norm_2')\n",
    "            layer_2 = tf.nn.relu(layer_2)\n",
    "            \n",
    "            network = tf.layers.dense(layer_2, self.num_outputs, name = 'output_layer')\n",
    "            network = tf.nn.sigmoid(network)\n",
    "            \n",
    "            # Scale output to action range\n",
    "            network = tf.add(tf.multiply(network, (self.output_max - self.output_min)), self.output_min)   \n",
    "        \n",
    "        # Add network variables to tf.collection for easy retrieval\n",
    "        with tf.variable_scope(scope, reuse = True):\n",
    "            \n",
    "            # Layer 1: Dense variables + Batch Norm variables\n",
    "            tf.add_to_collection(scope, tf.get_variable('dense_layer_1/kernel'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('dense_layer_1/bias'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('batch_norm_1/gamma'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('batch_norm_1/beta')) \n",
    "            \n",
    "            # Layer 2: Dense variables + Batch Norm variables\n",
    "            tf.add_to_collection(scope, tf.get_variable('dense_layer_2/kernel'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('dense_layer_2/bias'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('batch_norm_2/gamma'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('batch_norm_2/beta'))\n",
    "            \n",
    "            # Output layer: Dense variables\n",
    "            tf.add_to_collection(scope, tf.get_variable('output_layer/kernel'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('output_layer/bias'))\n",
    "            \n",
    "            # Non-Trainable Batch Norm Variables\n",
    "            tf.add_to_collection(scope, tf.get_variable('batch_norm_1/moving_mean'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('batch_norm_1/moving_variance'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('batch_norm_2/moving_mean'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('batch_norm_2/moving_variance'))\n",
    "            \n",
    "        return input, is_training, network  \n",
    "       \n",
    "        \n",
    "    def train(self, inputs, gradient):\n",
    "        self.session.run(self.optimizer, feed_dict = {self.actor_input: inputs, \n",
    "                                                      self.action_gradient: gradient, \n",
    "                                                      self.actor_is_training: True})\n",
    "       \n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.session.run(self.update_target_network_op)  \n",
    "    \n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.session.run(self.actor_network, feed_dict = {self.actor_input: state})\n",
    "    \n",
    "    \n",
    "    def predict_target(self, state):\n",
    "        return self.session.run(self.target_network, feed_dict = {self.target_input: state})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic Neural Network ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork:\n",
    "    \n",
    "    # Critic Network Parameters\n",
    "    num_outputs = 1\n",
    "    num_hidden_1 = 400\n",
    "    num_hidden_2 = 300\n",
    "    \n",
    "    def __init__(self, session, num_states, learning_rate, tau, batch_size):\n",
    "        \n",
    "        # Set session\n",
    "        self.session = session\n",
    "        \n",
    "        # Set input and output parameters\n",
    "        self.num_inputs = num_states\n",
    "        \n",
    "        # Set learning parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Create critic and target networks\n",
    "        self.critic_input, self.action, self.critic_network = self.make('critic')\n",
    "        self.target_input, self.target_action, self.target_network = self.make('target_critic')\n",
    "        \n",
    "        # Retrieve the collections with the variables of the actor and target networks\n",
    "        critic_collection = tf.get_collection('critic')\n",
    "        target_collection = tf.get_collection('target_critic')\n",
    "        \n",
    "        # Create update_target_network_op\n",
    "        self.update_target_network_op = [t.assign(tf.scalar_mul(self.tau, c) + tf.scalar_mul(1. - self.tau, t)) \n",
    "                                        for c, t in zip(critic_collection, target_collection)]\n",
    "        \n",
    "        # Target y_i values\n",
    "        self.target_q_value = tf.placeholder(tf.float32, [None, 1])\n",
    "        \n",
    "        # Loss and optimization \n",
    "        self.loss = tf.reduce_mean(tf.square(self.target_q_value - self.critic_network))\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        # Gradient with respect to actions\n",
    "        self.action_gradients_op = tf.gradients(self.critic_network, self.action)\n",
    "        \n",
    "\n",
    "    def make(self, scope):\n",
    "        \n",
    "        # Add variable scope to easily differentiate between actor and target networks\n",
    "        with tf.variable_scope(scope): \n",
    "            \n",
    "            # Define input placeholders\n",
    "            input = tf.placeholder(tf.float32, [None,self.num_inputs])\n",
    "            action = tf.placeholder(tf.float32, [None, 1])\n",
    "            is_training = tf.placeholder_with_default(False, [], name = 'is_training')\n",
    "\n",
    "            layer_1 = tf.layers.dense(input, self.num_hidden_1, name = 'dense_layer_1')\n",
    "            layer_1 = tf.layers.batch_normalization(layer_1, training = is_training, name = 'batch_norm_1')\n",
    "            layer_1 = tf.nn.relu(layer_1)\n",
    "            \n",
    "            layer_2 = tf.layers.dense(layer_1, self.num_hidden_2, name = 'dense_layer_2')\n",
    "            layer_2 = tf.layers.batch_normalization(layer_2, training = is_training, name = 'batch_norm_2')\n",
    "            layer_2 = tf.nn.relu(layer_2)\n",
    "            \n",
    "            network = tf.layers.dense(layer_2, self.num_outputs, name = 'output_layer')\n",
    "            \n",
    "        # Add network variables to tf.collection for easy retrieval\n",
    "        with tf.variable_scope(scope, reuse = True):\n",
    "            \n",
    "            # Layer 1: Dense variables + Batch Norm variables\n",
    "            tf.add_to_collection(scope, tf.get_variable('dense_layer_1/kernel'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('dense_layer_1/bias'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('batch_norm_1/gamma'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('batch_norm_1/beta'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('batch_norm_1/moving_mean'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('batch_norm_1/moving_variance'))\n",
    "            \n",
    "            # Layer 2: Dense variables + Batch Norm variables\n",
    "            tf.add_to_collection(scope, tf.get_variable('dense_layer_2/kernel'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('dense_layer_2/bias'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('batch_norm_2/gamma'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('batch_norm_2/beta'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('batch_norm_2/moving_mean'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('batch_norm_2/moving_variance'))\n",
    "            \n",
    "            # Output layer: Dense variables\n",
    "            tf.add_to_collection(scope, tf.get_variable('output_layer/kernel'))\n",
    "            tf.add_to_collection(scope, tf.get_variable('output_layer/bias'))\n",
    "        \n",
    "        return input, action, is_training, network\n",
    "        \n",
    "\n",
    "    def train(self, inputs, actions, target_q_values):\n",
    "        self.session.run([self.critic_network, self.optimizer], feed_dict = {\n",
    "            self.critic_input: inputs, \n",
    "            self.action: actions,\n",
    "            self.target_q_value: target_q_values,\n",
    "            self.critic_is_training: True})\n",
    "       \n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.session.run(self.update_target_network_op)  \n",
    "    \n",
    "    \n",
    "    def predict(self, states, actions):\n",
    "        return self.session.run(self.critic_network, feed_dict = {\n",
    "            self.critic_input: states,\n",
    "            self.action: actions})\n",
    "    \n",
    "    \n",
    "    def predict_target(self, states, actions):\n",
    "        return self.session.run(self.target_network, feed_dict = {\n",
    "            self.target_input: states,\n",
    "            self.target_action: actions})\n",
    "    \n",
    "    def action_gradients(self, states, actions):\n",
    "        return session.run(self.action_gradients_op, feed_dict = {\n",
    "            self.critic_input: states,\n",
    "            self.action: actions})[0]\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0/50000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-634a4592eaa2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;31m# Update target networks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-657f3383d456>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, inputs, gradient)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_gradient\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gym\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gym\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gym\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gym\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gym\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reset tf.Graph()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create action range\n",
    "action_range = np.append(env.action_space.low, env.action_space.high)\n",
    "\n",
    "\"\"\" Start TensorFlow session\"\"\"\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    # Create buffer\n",
    "    replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "\n",
    "    # Create networks\n",
    "    actor = ActorNetwork(session, num_states, action_range, LEARN_RATE_ACTOR, TAU, BATCH_SIZE)\n",
    "    critic = CriticNetwork(session, num_states, LEARN_RATE_CRITIC, TAU, BATCH_SIZE)\n",
    "    \n",
    "    # Initialize \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Epoch reward\n",
    "    episode_reward = np.zeros(EPOCHS)\n",
    "    \n",
    "    # Action noise generator\n",
    "    noise = ActionNoise()\n",
    "    \n",
    "    \"\"\" Epochs loop \"\"\"\n",
    "    for i in range(EPOCHS):\n",
    "        \n",
    "        # Reset the environment at the start of each epcoh\n",
    "        state = np.reshape(env.reset(), [num_states, 1])\n",
    "\n",
    "        if i % 10 == 0: print('EPOCH: {}/{}'.format(i, EPOCHS))\n",
    "        \"\"\" Steps in each epoch \"\"\"\n",
    "        for j in range(EPOCH_LENGTH):\n",
    "            \n",
    "            # Render the environment if specified\n",
    "            if RENDER:\n",
    "                env.render()\n",
    "                \n",
    "            \"\"\" Select action, execute and update buffer \"\"\"\n",
    "            # Determine action and critic values\n",
    "            action = actor.predict(state.reshape([1, num_states])) + noise()\n",
    "            \n",
    "            # Run the environment\n",
    "            next_state, reward, done, _info = env.step(action)\n",
    "\n",
    "            # Add experience to the replay buffer\n",
    "            replay_buffer.add(state, action, reward, done, next_state)\n",
    "\n",
    "            # Update state & reward\n",
    "            state = next_state\n",
    "            episode_reward[i] += reward\n",
    "            \n",
    "            \"\"\" Start Training \"\"\"\n",
    "            if replay_buffer.size() >= BATCH_SIZE:\n",
    "                # Replay batch\n",
    "                batch_state, batch_action, batch_reward, batch_done, batch_next_state = replay_buffer.batch(BATCH_SIZE)\n",
    "\n",
    "                # Predict value under target policy\n",
    "                target_q = critic.predict_target(batch_state, batch_action)\n",
    "\n",
    "                # Train critic\n",
    "                y_i = batch_reward + GAMMA*target_q\n",
    "                critic.train(batch_state, batch_action, y_i)\n",
    "\n",
    "                # Update actor\n",
    "                actions = actor.predict(batch_state)\n",
    "                gradients = critic.action_gradients(batch_state, actions)\n",
    "                actor.train(batch_state, gradients)\n",
    "\n",
    "                # Update target networks\n",
    "                actor.update_target_network()\n",
    "                critic.update_target_network()     \n",
    "                \n",
    "     \n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(EPOCHS), episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-71b172447cd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Determine action and critic values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_states\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Run the environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-bf0f0b74b116>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gym\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gym\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;31m# Check session.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Attempted to use a closed Session.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "for i in range(10000):\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "    # Determine action and critic values\n",
    "    action = actor.predict(state.reshape([1, num_states])) + noise()\n",
    "            \n",
    "    # Run the environment\n",
    "    next_state, reward, done, _info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Create checkpoint\n",
    "target_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(EPOCHS).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0/10\n"
     ]
    }
   ],
   "source": [
    "print('EPOCH: {}/{}'.format(0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset tf.Graph()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create action range\n",
    "action_range = np.append(env.action_space.low, env.action_space.high)\n",
    "\n",
    "\"\"\" Start TensorFlow session\"\"\"\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    # Create buffer\n",
    "    replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "\n",
    "    # Create networks\n",
    "    actor = ActorNetwork(session, num_states, action_range, LEARN_RATE_ACTOR, TAU, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ActorNetwork at 0x1987fdd8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'actor/dense_layer_1/kernel:0' shape=(3, 400) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_1/bias:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/gamma:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/beta:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_2/kernel:0' shape=(400, 300) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_2/bias:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/gamma:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/beta:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/output_layer/kernel:0' shape=(300, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/output_layer/bias:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables('actor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'actor/dense_layer_1/kernel:0' shape=(3, 400) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_1/bias:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/gamma:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/beta:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/moving_mean:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/moving_variance:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_2/kernel:0' shape=(400, 300) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_2/bias:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/gamma:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/beta:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/moving_mean:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/moving_variance:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/output_layer/kernel:0' shape=(300, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/output_layer/bias:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_1/kernel/Adam:0' shape=(3, 400) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_1/kernel/Adam_1:0' shape=(3, 400) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_1/bias/Adam:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_1/bias/Adam_1:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/gamma/Adam:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/gamma/Adam_1:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/beta/Adam:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/beta/Adam_1:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/moving_mean/Adam:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/moving_mean/Adam_1:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/moving_variance/Adam:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/moving_variance/Adam_1:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_2/kernel/Adam:0' shape=(400, 300) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_2/kernel/Adam_1:0' shape=(400, 300) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_2/bias/Adam:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_2/bias/Adam_1:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/gamma/Adam:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/gamma/Adam_1:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/beta/Adam:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/beta/Adam_1:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/moving_mean/Adam:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/moving_mean/Adam_1:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/moving_variance/Adam:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/moving_variance/Adam_1:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/output_layer/kernel/Adam:0' shape=(300, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/output_layer/kernel/Adam_1:0' shape=(300, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/output_layer/bias/Adam:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/output_layer/bias/Adam_1:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.global_variables('actor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'actor/dense_layer_1/kernel:0' shape=(3, 400) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_1/bias:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/gamma:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/beta:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/moving_mean:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_1/moving_variance:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_2/kernel:0' shape=(400, 300) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/dense_layer_2/bias:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/gamma:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/beta:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/moving_mean:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/batch_norm_2/moving_variance:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/output_layer/kernel:0' shape=(300, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'actor/output_layer/bias:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection('actor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "            # Build the network layer by layer\n",
    "            layer_1 = tf.add(tf.matmul(input, weights['hidden_1']), biases['hidden_1'])\n",
    "            layer_1 = tf.contrib.layers.batch_norm(layer_1, is_training = is_training)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
