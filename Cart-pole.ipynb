{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cart Pole Problem\n",
    "The goal of this project is to balance a pole on a cart which you can move back and forth.\n",
    "There are two possible actions, add force to left and add force to right. Scoring takes place for how long you keep the pole in the air\n",
    "\n",
    "I am going to attempt to implement (from scratch) a simple 1 fully connected hidden layer ANN with N nodes and 2 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 \n",
    "Define an \"Agent\" which has .run(), .update(), .reset() In this case the agent will be implemented in numpy for the practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, e_size, h_size, a_size, lr=0.01, gamma=0.99):\n",
    "        # Record Agent Parameters\n",
    "        self.e_size = e_size\n",
    "        self.h_size = h_size\n",
    "        self.a_size = a_size\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.hidden_weights = np.random.standard_normal((e_size, h_size))\n",
    "        self.hidden_bias = np.zeros((1, h_size))\n",
    "        \n",
    "        self.action_weights = np.random.standard_normal((h_size, a_size))\n",
    "        self.action_bias = np.zeros((1, a_size))\n",
    "        \n",
    "        # Initialize Replay history\n",
    "        self.history = []\n",
    "    \n",
    "    def reset(self):\n",
    "        self.history = []\n",
    "        \n",
    "    def activation(self, x):\n",
    "        return x\n",
    "    \n",
    "    def activation_derivative(self, x):\n",
    "        return 1\n",
    "    \n",
    "    def discountedReward(self, rewards):\n",
    "        for i in xrange(rewards.shape[0]-2, -1, -1):\n",
    "            rewards[i] = rewards[i] + self.gamma*rewards[i+1]\n",
    "        return rewards\n",
    "    \n",
    "    def record(self, s, a, r):\n",
    "        # Ensuring state s is a proper row vector\n",
    "        if (len(s.shape)==1):\n",
    "            s = s[None, :]\n",
    "            \n",
    "        self.history.append([s, a, r])\n",
    "    \n",
    "    def forward_pass(self, s):\n",
    "        # Implement the forward pass returning the various intermediate results\n",
    "        \n",
    "        # Check the observation has the correct number of dimensions\n",
    "        assert (len(s.shape) <= 2 and len(s.shape) != 0)\n",
    "        # If it has one dimension turn into a row vector\n",
    "        if (len(s.shape)==1):\n",
    "            s = s[None,:]\n",
    "            \n",
    "        # If it has 2 dimensions ensure it is a vector not a matrix\n",
    "        assert (s.shape[0] == 1 or s.shape[1] == 1)\n",
    "        # Convert to row matrix if its a column\n",
    "        if (s.shape[0] !=1):\n",
    "            s = s.T\n",
    "            \n",
    "        # Input Checking complete. Run forward pass\n",
    "        hidden_input = np.dot(s, self.hidden_weights) + self.hidden_bias\n",
    "        hidden_output = self.activation(hidden_input)\n",
    "        \n",
    "        action_input = np.dot(hidden_output, self.action_weights) + self.action_bias\n",
    "        action_output = self.activation(action_input)\n",
    "        \n",
    "        return [hidden_input, hidden_output, action_input, action_output]\n",
    "    \n",
    "    def update(self):\n",
    "        # Function to use back propagation with the history to improve weights\n",
    "        \n",
    "        # Input checking\n",
    "        assert len(self.history) > 0\n",
    "        \n",
    "        # Applying discounted reward to this particular replay\n",
    "        history = np.array(self.history)\n",
    "        history[:,2] = self.discountedReward(history[:,2])\n",
    "        \n",
    "        # Prepare for gradients\n",
    "        HWChange = np.zeros(self.hidden_weights.shape)\n",
    "        HBChange = np.zeros(self.hidden_bias.shape)\n",
    "        \n",
    "        AWChange = np.zeros(self.action_weights.shape)\n",
    "        ABChange = np.zeros(self.action_bias.shape)\n",
    "        for i in xrange(history.shape[0]):\n",
    "            # For each step of history:\n",
    "                # Apply forward pass\n",
    "                # \n",
    "            state = history[i, 0]\n",
    "            action = history[i, 1]\n",
    "            reward = history[i, 2]\n",
    "            \n",
    "#             print ((state, action, reward))\n",
    "            \n",
    "            (HI, H, A, yhat) = self.forward_pass(state)\n",
    "#             print(HI)\n",
    "#             print(H)\n",
    "#             print(A)\n",
    "#             print(\"++++++++++++++++++++++\")\n",
    "#             print (yhat)\n",
    "#             print (reward)\n",
    "#             print (-1*(reward-yhat[0,action]))\n",
    "#             print (\"--------------\")\n",
    "            \n",
    "            main_error_term = np.zeros((1, self.a_size))\n",
    "            main_error_term[0, action] = -1*(reward-yhat[0,action])\n",
    "            \n",
    "            main_error_term = main_error_term*self.activation_derivative(A)\n",
    "            \n",
    "            hidden_error_term = np.dot(main_error_term, self.action_weights.T)\n",
    "            hidden_error_term = hidden_error_term*self.activation_derivative(HI)\n",
    "            \n",
    "#             print(\"\\n\")\n",
    "#             print (main_error_term.shape)\n",
    "#             print (main_error_term)\n",
    "#             print (hidden_error_term.shape)\n",
    "#             print (hidden_error_term)\n",
    "#             print (\"_________________________\")\n",
    "#             print (np.dot(state.T, hidden_error_term))\n",
    "#             print (self.hidden_weights)\n",
    "#             print (\"\\n\")\n",
    "            \n",
    "            AWChange += np.dot(H.T, main_error_term)\n",
    "            ABChange += main_error_term*1\n",
    "            \n",
    "            HWChange += np.dot(state.T, hidden_error_term)\n",
    "            HBChange += hidden_error_term*1\n",
    "        \n",
    "        self.hidden_weights -= self.lr*HWChange/history.shape[0]\n",
    "        self.hidden_bias -= self.lr*HBChange/history.shape[0]\n",
    "        \n",
    "        self.action_weights -= self.lr*AWChange/history.shape[0]\n",
    "        self.action_bias -= self.lr*ABChange/history.shape[0]\n",
    "        \n",
    "        # Delete history\n",
    "        self.history = []\n",
    "                \n",
    "    def predict(self, s):\n",
    "        # Return the Q values prdicted for input state S\n",
    "        return self.forward_pass(s)[-1]\n",
    "    \n",
    "    def choose_action(self, s):\n",
    "        # Use the prediction to make a decision\n",
    "        return np.argmax(self.predict(s))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Agent\n",
    "Implement tests for the back-propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Tests Passed!\n"
     ]
    }
   ],
   "source": [
    "e_size = 2\n",
    "h_size = 3\n",
    "a_size = 2\n",
    "\n",
    "init_h_weights = np.array([[0.1, 0.5, 0.2], [0.3, -0.1, 0.6]])\n",
    "init_h_bias = np.zeros((1,3))\n",
    "init_a_weights = np.array([[-0.1, 0.1], [0.4, 0.3], [0.2, 0]])\n",
    "init_a_bias = np.zeros((1,2))\n",
    "\n",
    "exp_h_weights = np.array([[0.146, 0.638, 0.2],[0.392, 0.176, 0.6]])\n",
    "exp_h_bias = np.array([0.0092, 0.0276, 0])\n",
    "exp_a_weights = np.array([[-0.1, 0.422],[0.4, 0.438],[0.2, 0.644]])\n",
    "exp_a_bias = np.array([0, 0.092])\n",
    "\n",
    "state = np.array([5, 10])\n",
    "action = 1\n",
    "reward = 10\n",
    "\n",
    "test_agent = agent(e_size, h_size, a_size, lr=0.01, gamma=0.99)\n",
    "\n",
    "test_agent.hidden_weights = init_h_weights\n",
    "test_agent.hidden_bias = init_h_bias\n",
    "test_agent.action_weights = init_a_weights\n",
    "test_agent.action_bias = init_a_bias\n",
    "\n",
    "test_agent.record(state, action, reward)\n",
    "\n",
    "test_agent.update()\n",
    "\n",
    "assert(np.allclose(test_agent.hidden_weights, exp_h_weights))\n",
    "assert(np.allclose(test_agent.hidden_bias, exp_h_bias))\n",
    "assert(np.allclose(test_agent.action_weights, exp_a_weights))\n",
    "assert(np.allclose(test_agent.action_bias, exp_a_bias))\n",
    "\n",
    "print(\"All Tests Passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Now that we have the agent we can start using it to play the game. This section we run through a bunch of games (restart when hit done) and after each game we run the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute the softmax of vector x.\"\"\"\n",
    "    exps = np.exp(x)\n",
    "    return exps / np.sum(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-22 22:46:42,150] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:       0    Score:   10.00    Play-time:     9\n",
      "Round:    1000    Score:    9.00    Play-time:     8\n",
      "Round:    2000    Score:   11.00    Play-time:    10\n",
      "Round:    3000    Score:   10.00    Play-time:     9\n",
      "Round:    4000    Score:   10.00    Play-time:     9\n",
      "Round:    5000    Score:   10.00    Play-time:     9\n",
      "Round:    6000    Score:   10.00    Play-time:     9\n",
      "Round:    7000    Score:   11.00    Play-time:    10\n",
      "Round:    8000    Score:   10.00    Play-time:     9\n",
      "Round:    9000    Score:    9.00    Play-time:     8\n",
      "Round:   10000    Score:   10.00    Play-time:     9\n",
      "Round:   11000    Score:    9.00    Play-time:     8\n",
      "Round:   12000    Score:   10.00    Play-time:     9\n",
      "Round:   13000    Score:    9.00    Play-time:     8\n",
      "Round:   14000    Score:   11.00    Play-time:    10\n",
      "Round:   15000    Score:   10.00    Play-time:     9\n",
      "Round:   16000    Score:   13.00    Play-time:    12\n",
      "Round:   17000    Score:   11.00    Play-time:    10\n",
      "Round:   18000    Score:    9.00    Play-time:     8\n",
      "Round:   19000    Score:    9.00    Play-time:     8\n",
      "Round:   20000    Score:   10.00    Play-time:     9\n",
      "Round:   21000    Score:   10.00    Play-time:     9\n",
      "Round:   22000    Score:    8.00    Play-time:     7\n",
      "Round:   23000    Score:    9.00    Play-time:     8\n",
      "Round:   24000    Score:    9.00    Play-time:     8\n",
      "Round:   25000    Score:   10.00    Play-time:     9\n",
      "Round:   26000    Score:   10.00    Play-time:     9\n",
      "Round:   27000    Score:    9.00    Play-time:     8\n",
      "Round:   28000    Score:    9.00    Play-time:     8\n",
      "Round:   29000    Score:    8.00    Play-time:     7\n",
      "Round:   30000    Score:    9.00    Play-time:     8\n",
      "Round:   31000    Score:   11.00    Play-time:    10\n",
      "Round:   32000    Score:   10.00    Play-time:     9\n",
      "Round:   33000    Score:   10.00    Play-time:     9\n",
      "Round:   34000    Score:   10.00    Play-time:     9\n",
      "Round:   35000    Score:    8.00    Play-time:     7\n",
      "Round:   36000    Score:    9.00    Play-time:     8\n",
      "Round:   37000    Score:   10.00    Play-time:     9\n",
      "Round:   38000    Score:    9.00    Play-time:     8\n",
      "Round:   39000    Score:   10.00    Play-time:     9\n",
      "Round:   40000    Score:    9.00    Play-time:     8\n",
      "Round:   41000    Score:    9.00    Play-time:     8\n",
      "Round:   42000    Score:   10.00    Play-time:     9\n",
      "Round:   43000    Score:    9.00    Play-time:     8\n",
      "Round:   44000    Score:   10.00    Play-time:     9\n",
      "Round:   45000    Score:   10.00    Play-time:     9\n",
      "Round:   46000    Score:   10.00    Play-time:     9\n",
      "Round:   47000    Score:   10.00    Play-time:     9\n",
      "Round:   48000    Score:    9.00    Play-time:     8\n",
      "Round:   49000    Score:   10.00    Play-time:     9\n",
      "Round:   49999    Score:   11.00    Play-time:    10\n"
     ]
    }
   ],
   "source": [
    "# Training Parameter\n",
    "MAX_GAMES = 50000 # This is like epochs\n",
    "MAX_GAME_TIME = 999\n",
    "\n",
    "LEARNING_RATE = 0.00001\n",
    "GAMMA = 0.99\n",
    "HIDDEN_NODES = 10\n",
    "\n",
    "REPORT_INTERVAL = 1000 # How often to report the progress.\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "e_size = 4\n",
    "a_size = 2\n",
    "player = agent(e_size, HIDDEN_NODES, a_size, lr=LEARNING_RATE, gamma=GAMMA)\n",
    "\n",
    "for i_episode in range(MAX_GAMES):\n",
    "    state = env.reset()\n",
    "    player.reset()\n",
    "    running_reward = 0\n",
    "    \n",
    "    for t in range(MAX_GAME_TIME):\n",
    "        \n",
    "        Q_vals = player.predict(state)\n",
    "        # Do a weighted sample to pick your action\n",
    "        action_prob = softmax(Q_vals[0,:])\n",
    "        action = np.random.choice(range(a_size), p=action_prob)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        player.record(state, action, reward)\n",
    "        running_reward += reward\n",
    "        state = new_state\n",
    "        if done:\n",
    "            player.update()\n",
    "            if (i_episode % REPORT_INTERVAL == 0 or i_episode == MAX_GAMES-1):\n",
    "                print(\"Round: {0:7d}    Score: {1:7.2f}    Play-time: {2:5d}\".format(i_episode, running_reward, t))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 12 timesteps\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "t = 0\n",
    "while t<900:\n",
    "    env.render()\n",
    "    action = player.choose_action(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    t += 1\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RL]",
   "language": "python",
   "name": "conda-env-RL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
