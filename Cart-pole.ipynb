{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cart Pole Problem\n",
    "The goal of this project is to balance a pole on a cart which you can move back and forth.\n",
    "There are two possible actions, add force to left and add force to right. Scoring takes place for how long you keep the pole in the air\n",
    "\n",
    "I am going to attempt to implement (from scratch) a simple 1 fully connected hidden layer ANN with N nodes and 2 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 \n",
    "Define an \"Agent\" which has .run(), .update(), .reset() In this case the agent will be implemented in numpy for the practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation:\n",
    "    def __init__(self, type='linear'):\n",
    "        self.type = type\n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1/(1+np.exp(x))\n",
    "    \n",
    "    def activate(self, x):\n",
    "        if self.type == 'linear':\n",
    "            return x\n",
    "        if self.type == 'relu':\n",
    "            mask = x > 0\n",
    "            return mask*x\n",
    "        if self.type == 'sigmoid':\n",
    "            return self._sigmoid(x)\n",
    "        \n",
    "    def derivative(self, x):\n",
    "        if self.type == 'linear':\n",
    "            return 1\n",
    "        if self.type == 'relu':\n",
    "            return 1*(x>0)\n",
    "        if self.type == 'sigmoid':\n",
    "            return self._sigmoid(x)*(1-self._sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, e_size, h_size, a_size, lr=0.01, gamma=0.99, a1='linear', a2='linear', discount='exp'):\n",
    "        # Record Agent Parameters\n",
    "        self.e_size = e_size\n",
    "        self.h_size = h_size\n",
    "        self.a_size = a_size\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.A1 = activation(a1)\n",
    "        self.A2 = activation(a2)\n",
    "        self.discount = discount\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.hidden_weights = np.random.standard_normal((e_size, h_size))\n",
    "        self.hidden_bias = np.zeros((1, h_size))\n",
    "        \n",
    "        self.action_weights = np.random.standard_normal((h_size, a_size))\n",
    "        self.action_bias = np.zeros((1, a_size))\n",
    "        \n",
    "        # Initialize Replay history\n",
    "        self.history = []\n",
    "    \n",
    "    def reset(self):\n",
    "        self.history = []\n",
    "    \n",
    "    def discountedReward(self, rewards):\n",
    "        if self.discount == 'exp':\n",
    "            for i in xrange(rewards.shape[0]-2, -1, -1):\n",
    "                rewards[i] = rewards[i] + self.gamma*rewards[i+1]\n",
    "        elif (self.discount == 'lin'):\n",
    "            for i in xrange(rewards.shape[0]):\n",
    "                rewards[i] = min(np.sum(rewards[i:]), 30)\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    def record(self, s, a, r):\n",
    "        # Ensuring state s is a proper row vector\n",
    "        if (len(s.shape)==1):\n",
    "            s = s[None, :]\n",
    "            \n",
    "        self.history.append([s, a, r])\n",
    "    \n",
    "    def forward_pass(self, s):\n",
    "        # Implement the forward pass returning the various intermediate results\n",
    "        \n",
    "        # Check the observation has the correct number of dimensions\n",
    "        assert (len(s.shape) <= 2 and len(s.shape) != 0)\n",
    "        # If it has one dimension turn into a row vector\n",
    "        if (len(s.shape)==1):\n",
    "            s = s[None,:]\n",
    "            \n",
    "        # If it has 2 dimensions ensure it is a vector not a matrix\n",
    "        assert (s.shape[0] == 1 or s.shape[1] == 1)\n",
    "        # Convert to row matrix if its a column\n",
    "        if (s.shape[0] !=1):\n",
    "            s = s.T\n",
    "            \n",
    "        # Input Checking complete. Run forward pass\n",
    "        hidden_input = np.dot(s, self.hidden_weights) + self.hidden_bias\n",
    "        hidden_output = self.A1.activate(hidden_input)\n",
    "        \n",
    "        action_input = np.dot(hidden_output, self.action_weights) + self.action_bias\n",
    "        action_output = self.A2.activate(action_input)\n",
    "        \n",
    "        return [hidden_input, hidden_output, action_input, action_output]\n",
    "    \n",
    "    def update(self):\n",
    "#         print (\"UPDATING\")\n",
    "        # Function to use back propagation with the history to improve weights\n",
    "        \n",
    "        # Input checking\n",
    "        assert len(self.history) > 0\n",
    "        \n",
    "        # Applying discounted reward to this particular replay\n",
    "        history = np.array(self.history)\n",
    "        history[:,2] = self.discountedReward(history[:,2])\n",
    "        \n",
    "        # Prepare for gradients\n",
    "        HWChange = np.zeros(self.hidden_weights.shape)\n",
    "        HBChange = np.zeros(self.hidden_bias.shape)\n",
    "        \n",
    "        AWChange = np.zeros(self.action_weights.shape)\n",
    "        ABChange = np.zeros(self.action_bias.shape)\n",
    "        for i in xrange(history.shape[0]):\n",
    "            # For each step of history:\n",
    "                # Apply forward pass\n",
    "                # \n",
    "            state = history[i, 0]\n",
    "            action = history[i, 1]\n",
    "            reward = history[i, 2]\n",
    "            \n",
    "            (HI, H, A, yhat) = self.forward_pass(state)\n",
    "            \n",
    "            main_error_term = np.zeros((1, self.a_size))\n",
    "            main_error_term[0, action] = -1*(reward-yhat[0,action])\n",
    "            \n",
    "            main_error_term = main_error_term*self.A1.derivative(A)\n",
    "            \n",
    "            hidden_error_term = np.dot(main_error_term, self.action_weights.T)\n",
    "            hidden_error_term = hidden_error_term*self.A2.derivative(HI)\n",
    "            \n",
    "            AWChange += np.dot(H.T, main_error_term)\n",
    "            ABChange += main_error_term*1\n",
    "            \n",
    "            HWChange += np.dot(state.T, hidden_error_term)\n",
    "            HBChange += hidden_error_term*1\n",
    "        \n",
    "        run_length = 1#10*np.sqrt(history.shape[0])\n",
    "        \n",
    "        self.hidden_weights -= self.lr*HWChange/run_length\n",
    "        self.hidden_bias -= self.lr*HBChange/run_length\n",
    "        \n",
    "        self.action_weights -= self.lr*AWChange/run_length\n",
    "        self.action_bias -= self.lr*ABChange/run_length\n",
    "        \n",
    "        # Delete history\n",
    "        self.history = []\n",
    "                \n",
    "    def predict(self, s):\n",
    "        # Return the Q values prdicted for input state S\n",
    "        return self.forward_pass(s)[-1]\n",
    "    \n",
    "    def choose_action(self, s):\n",
    "        # Use the prediction to make a decision\n",
    "        return np.argmax(self.predict(s))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Agent\n",
    "Implement tests for the back-propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Tests Passed!\n"
     ]
    }
   ],
   "source": [
    "e_size = 2\n",
    "h_size = 3\n",
    "a_size = 2\n",
    "\n",
    "init_h_weights = np.array([[0.1, 0.5, 0.2], [0.3, -0.1, 0.6]])\n",
    "init_h_bias = np.zeros((1,3))\n",
    "init_a_weights = np.array([[-0.1, 0.1], [0.4, 0.3], [0.2, 0]])\n",
    "init_a_bias = np.zeros((1,2))\n",
    "\n",
    "exp_h_weights = np.array([[0.146, 0.638, 0.2],[0.392, 0.176, 0.6]])\n",
    "exp_h_bias = np.array([0.0092, 0.0276, 0])\n",
    "exp_a_weights = np.array([[-0.1, 0.422],[0.4, 0.438],[0.2, 0.644]])\n",
    "exp_a_bias = np.array([0, 0.092])\n",
    "\n",
    "state = np.array([5, 10])\n",
    "action = 1\n",
    "reward = 10\n",
    "\n",
    "test_agent = agent(e_size, h_size, a_size, lr=0.01, gamma=0.99)\n",
    "\n",
    "test_agent.hidden_weights = init_h_weights\n",
    "test_agent.hidden_bias = init_h_bias\n",
    "test_agent.action_weights = init_a_weights\n",
    "test_agent.action_bias = init_a_bias\n",
    "\n",
    "test_agent.record(state, action, reward)\n",
    "\n",
    "test_agent.update()\n",
    "\n",
    "assert(np.allclose(test_agent.hidden_weights, exp_h_weights))\n",
    "assert(np.allclose(test_agent.hidden_bias, exp_h_bias))\n",
    "assert(np.allclose(test_agent.action_weights, exp_a_weights))\n",
    "assert(np.allclose(test_agent.action_bias, exp_a_bias))\n",
    "\n",
    "print(\"All Tests Passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Now that we have the agent we can start using it to play the game. This section we run through a bunch of games (restart when hit done) and after each game we run the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute the softmax of vector x.\"\"\"\n",
    "    exps = np.exp(x-x.mean())\n",
    "    return exps / np.sum(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-23 12:49:59,116] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:       0   Avg Score:   16.00    Max Score: 16.000000\n",
      "Round:    5000   Avg Score:   11.42    Max Score: 31.000000\n",
      "Round:   10000   Avg Score:   30.26    Max Score: 153.000000\n",
      "Round:   15000   Avg Score:   32.92    Max Score: 173.000000\n",
      "Round:   20000   Avg Score:   35.73    Max Score: 200.000000\n",
      "Round:   25000   Avg Score:   35.86    Max Score: 180.000000\n",
      "Round:   30000   Avg Score:   36.58    Max Score: 164.000000\n",
      "Round:   35000   Avg Score:   37.27    Max Score: 200.000000\n",
      "Round:   40000   Avg Score:   36.55    Max Score: 196.000000\n",
      "Round:   45000   Avg Score:   36.10    Max Score: 200.000000\n",
      "Round:   50000   Avg Score:   36.39    Max Score: 200.000000\n",
      "Round:   55000   Avg Score:   35.38    Max Score: 182.000000\n",
      "Round:   60000   Avg Score:   35.93    Max Score: 190.000000\n",
      "Round:   65000   Avg Score:   35.67    Max Score: 179.000000\n",
      "Round:   70000   Avg Score:   34.78    Max Score: 194.000000\n",
      "Round:   75000   Avg Score:   34.59    Max Score: 186.000000\n",
      "Round:   80000   Avg Score:   34.99    Max Score: 200.000000\n",
      "Round:   85000   Avg Score:   35.46    Max Score: 188.000000\n",
      "Round:   90000   Avg Score:   35.78    Max Score: 190.000000\n",
      "Round:   95000   Avg Score:   35.93    Max Score: 200.000000\n",
      "Round:   99999   Avg Score:   34.60    Max Score: 182.000000\n"
     ]
    }
   ],
   "source": [
    "# Training Parameter\n",
    "MAX_GAMES = 100000 # This is like epochs\n",
    "MAX_GAME_TIME = 999\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "DECAY = 1\n",
    "# DECAY = 0.99996\n",
    "GAMMA = 0.8576\n",
    "HIDDEN_NODES = 10\n",
    "\n",
    "REPORT_INTERVAL = 5000 # How often to report the progress.\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "e_size = 4\n",
    "a_size = 2\n",
    "player = agent(e_size, HIDDEN_NODES, a_size, lr=LEARNING_RATE, gamma=GAMMA, a1='relu')\n",
    "\n",
    "avgScore = 0.\n",
    "runCount = 0.\n",
    "bestScore = 0\n",
    "\n",
    "for i_episode in range(MAX_GAMES):\n",
    "    state = env.reset()\n",
    "    player.reset()\n",
    "    running_reward = 0\n",
    "    \n",
    "    for t in range(MAX_GAME_TIME):\n",
    "        \n",
    "        Q_vals = player.predict(state)\n",
    "        # Do a weighted sample to pick your action\n",
    "        action_prob = softmax(Q_vals[0,:])\n",
    "        action = np.random.choice(range(a_size), p=action_prob)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        player.record(state, action, reward)\n",
    "        running_reward += reward\n",
    "        state = new_state\n",
    "        if done:\n",
    "            avgScore += running_reward\n",
    "            runCount += 1\n",
    "            if (running_reward > bestScore):\n",
    "                bestScore = running_reward\n",
    "            player.update()\n",
    "            if (i_episode > MAX_GAMES/2.0):\n",
    "                player.lr = 0.0001 #player.lr*DECAY\n",
    "            if (i_episode % REPORT_INTERVAL == 0 or i_episode == MAX_GAMES-1):\n",
    "                print(\"Round: {0:7d}   Avg Score: {1:7.2f}    Max Score: {2:5f}\".format(i_episode, avgScore/runCount, bestScore))\n",
    "                avgScore = 0.\n",
    "                runCount = 0.\n",
    "                bestScore = 0\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-23 13:25:36,821] Making new env: CartPole-v0\n",
      "[2017-11-23 13:25:36,826] Starting new video recorder writing to /home/dennis/Documents/ML/ReinforcementLearning/cart-pole-monitor/openaigym.video.7.9899.video000000.mp4\n",
      "[2017-11-23 13:25:43,744] Starting new video recorder writing to /home/dennis/Documents/ML/ReinforcementLearning/cart-pole-monitor/openaigym.video.7.9899.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 201 timesteps\n"
     ]
    }
   ],
   "source": [
    "from gym.wrappers import Monitor\n",
    "env = gym.make('CartPole-v0')\n",
    "env = Monitor(env, './cart-pole-monitor')\n",
    "state = env.reset()\n",
    "done = False\n",
    "t = 0\n",
    "while t<200:\n",
    "#     env.render()\n",
    "    action = player.choose_action(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    t += 1\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "env.reset()\n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "Being my first reinforcement learning project I wanted to implement it from scratch using numpy (because numpy is a bit easier to debug then tensorflow)\n",
    "\n",
    "On the whole this was very enlightening. I decided to go for a DQN which predicts the actual Q values rather than one which predicts actions directly simply because the Q prediction model made a lot more sense to me. When you get a reward you try to update the network to better reflect what the expected reward for that action is. Beautiful in its simplicity.\n",
    "\n",
    "My biggest challenge was getting the model to converge. Even now it is somewhat hit-and miss. Mostly it seemed like early on it needed a larger learning rate but as it when on it needed to fine tune. That said sometimes it seems like it would get stuck in a rut right from the start and not be able to break out.\n",
    "\n",
    "I am sure that there are work-arounds for this the two ideas I tried is learning rate decay both exponential decay and discrete value decay. Both had improvements though both would still exhibit finding a peak and then falling off\n",
    "\n",
    "I suspected that the reason was that the early states tend to be similar and sometimes good courses of action were being unfairly punished early on because it was having trouble learning. As such I tried normalizing by the sqrt(length of the run) thinking this would allow more movement to remember good moves more easily and not as heavily punish bad moves. (Carrot vs Stick) \n",
    "\n",
    "It appears to be successful because the few tests I have done with it have converged much easier, but testing the bot doesn't seem te reflect the performance I was expecting. The average score is not great but the max is. I suspect that while possibly in the right direction, the is too much Carrot right now :P\n",
    "\n",
    "In the end I have decided to move on and do some more reading. As a first attempt it did pretty good but on the whole I think I am running into some wall i need to better understand.\n",
    "\n",
    "It occurs to me that if the same state is reached but at different times in the run then the target reward will be vastly different... Maybe using discounted reward is not the best policy in this case since the only thing that gives reward is staying in the air longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional results\n",
    "Continuing in the investigation I realized I could tune the carrot stick ratio by adding a multiplicative factor out front of the sqrt. Ultimately if this works I think the same effect could be produced by a linear function but thats an investigation for another time.\n",
    "\n",
    "When Using the sqrt(run_length) as the normalizing factor the behaviour observed was that the bot would have a high max score for each reporting interval (often 200) but the average score remained around 30-40 This suggests to me that it was learning only one good path and was woefully unprepared for other starting states.\n",
    "\n",
    "The hope is that by normalizing by 10*sqrt(run_length) we will more evenly balance the carrot-stick ratio (centered on 100 as the crossing point.\n",
    "\n",
    "Its also worth noting that the avg score might be a poor metric. there is some randomness in the motions during training to avoid exploitation. to get a better metric we should really be testing the game properly.\n",
    "\n",
    "To be honest though I don't have enough experience at this point to really understand the implications here but it seems like maybe there is something to the idea.\n",
    "\n",
    "as for using the reward as the final reward... That idea didn't seem to pan out. When testing it it immediately dropped the quality of results. Visual testing seems like it learned one direction too well and ignored other situations. Maybe a constant reward if it wins for the next X moves and then a taper after that. A discrete convergent dynamical system could be good there...\n",
    "\n",
    "Interestingly it seems like normalizing the reward structure has helped a lot. It kind of makes sense. actions which will will help keep the pole in the air for a period greater then the typical influence time (i estimate about 30 steps though it might be less) all give the same reward. My only concernis that the linear dropoff is not a sufficient penalty for making mistakes.\n",
    "\n",
    "### Final thoughts\n",
    "I think what I took from this more than anything else is that the way we discount rewards has the biggest impact on the performance of all the factors. Ultimately I think using smarter optimizers will help but we can't escape the fact that how we consider the impact of an action on future rewards is probably the most critical aspect of this problem. However I also think that this could be coded in the game side if care is taken. for instance if we assume exponential decay of rewards we can tailor the the rewards being given to be more constant if the game is to remain constant. It doesn't make a lot of sense to give a reward for an action much larger just because that particular run went longer. The effect of that action ultimately has a time period of influence. \n",
    "\n",
    "HOWEVER with proper gamma tuning I think we can still do quite a bit. For instance in this case reducing gamma drastically should mean that some effective window of future events is considered. Lets say we consider 0.01 to be negliable then 0.01=gamma^N. If we want the effective period to be 30 time steps then gamma = 0.85 would be appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niCQJFbqISJJQoYuIJIn/AnAC0iUNLQOgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f084f12f2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.ones(60)\n",
    "for i in xrange(a.shape[0]):\n",
    "    x = np.sum(a[i:])\n",
    "    if (x >= 30):\n",
    "        a[i] = 30\n",
    "    else:\n",
    "        a[i] = 30*((-x/30)**4)\n",
    "#         a[i] = 60*(1/(1+np.exp((30-x)/5.0))\n",
    "\n",
    "plt.plot(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f084f104a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.ones(60)\n",
    "gamma = 0.8576\n",
    "for i in xrange(a.shape[0]-2, -1, -1):\n",
    "    a[i] = a[i] + gamma*a[i+1]\n",
    "\n",
    "plt.plot(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RL]",
   "language": "python",
   "name": "conda-env-RL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
